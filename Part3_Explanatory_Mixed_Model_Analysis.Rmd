---
title: "Explanatory Mixed Model Analysis"
author: "Ilse van Beelen & Floor Komen"
date: "11/4/2019"
output: html_document
---

```{r setup, include=FALSE, echo=TRUE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = "hide" )
```


Goal: create a linear mixed model to describe/explain the increase/decrease of NDI over time. Here NDI is the outcome and the predictors are time, HADS anx and HADS depres

# Set up

```{r, include=FALSE}
# Load ggCaterpillar, used to plot the random intercept
rm(list = ls())
set.seed(19950306)
library(ggplot2)
library(lme4)
library(tidyverse)
library(nlme)
library(emmeans)
library(car)
library(lattice)
library(xtable)
library(gridExtra)
library(groupdata2)


ggCaterpillar <- function(re, QQ=TRUE, likeDotplot=TRUE) {
    require(ggplot2)
    f <- function(x) {
        pv   <- attr(x, "postVar")
        cols <- 1:(dim(pv)[1])
        se   <- unlist(lapply(cols, function(i) sqrt(pv[i, i, ])))
        ord  <- unlist(lapply(x, order)) + rep((0:(ncol(x) - 1)) * nrow(x), each=nrow(x))
        pDf  <- data.frame(y=unlist(x)[ord],
                           ci=1.96*se[ord],
                           nQQ=rep(qnorm(ppoints(nrow(x))), ncol(x)),
                           ID=factor(rep(rownames(x), ncol(x))[ord], levels=rownames(x)[ord]),
                           ind=gl(ncol(x), nrow(x), labels=names(x)))

        if(QQ) {  ## normal QQ-plot
            p <- ggplot(pDf, aes(nQQ, y))
            p <- p + facet_wrap(~ ind, scales="free")
            p <- p + xlab("Standard normal quantiles") + ylab("Random effect quantiles")
        } else {  ## caterpillar dotplot
            p <- ggplot(pDf, aes(ID, y)) + coord_flip()
            if(likeDotplot) {  ## imitate dotplot() -> same scales for random effects
                p <- p + facet_wrap(~ ind)
            } else {           ## different scales for random effects
                p <- p + facet_grid(ind ~ ., scales="free_y")
            }
            p <- p + xlab("Levels") + ylab("Random effects")
        }

        p <- p + theme(legend.position="none")
        p <- p + geom_hline(yintercept=0)
        p <- p + geom_errorbar(aes(ymin=y-ci, ymax=y+ci), width=0, colour="black")
        p <- p + geom_point(aes(size=1.2), colour="blue") 
        return(p)
    }

    lapply(re, f)
}

# Set layout for all figures
theme <- theme(panel.background = element_blank(),
          panel.grid.major = element_line(colour = "darkgrey", size=0.5),
          panel.grid.minor = element_line(colour = "grey", 
                                          size=.25, 
                                          linetype = "dashed"),
          panel.border = element_blank(),
          axis.line.x = element_line(colour = "black", 
                                     size=0.5, 
                                     lineend = "butt"),
          axis.line.y = element_line(colour = "black", 
                                     size=0.5),
          axis.text=element_text(size=15),
              axis.title=element_text(size=22),
              plot.title = element_text(size = 22),
              strip.text = element_text(size = 15),
              legend.title = element_blank())


```



```{r, results='hide'}

# Load libraries and read in data
df_long <- read.csv("Data/data_final_2019-11-05.csv", sep = ",")

# Create time factor & center HADS at its mean
df_long$time_fct <- as.factor(df_long$time_fct)
df_long$hads_tot_cnt <- df_long$hads_tot - mean(df_long$hads_tot)
str(df_long)

```




# Linear Mixed Model Analysis

We tested several models. HADS anx and depres are highly correlated ($R^2 = 0.8$). If you add them together, one is suddenly no longer significant. It is best to combine them in `hads_tot`, since they explain to same information.

```{r}

model1 <- nlme::lme(ndi ~ time_fct, random = ~1|id, method= "ML", data = df_long)
model2 <- nlme::lme(ndi ~ time_fct + hads_anx, random = ~1|id, method= "ML", data = df_long)
model3 <- nlme::lme(ndi ~ time_fct + hads_depr, random = ~1|id, method= "ML", 
                    data = df_long)
model4 <- nlme::lme(ndi ~ time_fct  + hads_depr + hads_anx, random = ~1|id, method= "ML", 
                    data = df_long)
model5 <- nlme::lme(ndi ~ time_fct + hads_tot, random = ~1|id, method= "ML", 
                    data = df_long)

anova(model1, model2)
anova(model2, model3)
anova(model4, model5)
summary(model5)



```

## Check multicollinearity

There is a strong correlation between HADS anxiety and HADS depression. We now check for multicollinearity. No VIF above 5 (thus nu R-squared above 0.9)

```{r}
vif(model5)

```


## Check assumptions

We assume the folowing:

- error term is normally distributed
- error term is homogeneously distributed
- no outliers

```{r}

# Homogeneity of variance
par(mfrow = (c(1,3)))
plot(resid(model5))
df_long[c(34, 289), ]

# Normally distributed residuals
qqPlot(resid(model5))

# Influencial points
infl2 = influence(model5, obs = T)
cooksd = cooks.distance(infl2) # two outliers, 


plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x = 1:length(cooksd) , y=cooksd, labels=ifelse(cooksd> 4*mean(cooksd, na.rm = T),
                                                    1:length(cooksd),""), col="blue", pos =2)

df_long[c(251, 27, 287, 143), ]



```


# Fit different random effects

We now compare a random intercept model to random intercept + slope

```{r}

model_int <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
                         random = ~1|id, data = df_long)

model_int_slope <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
                         random = ~1+ time_fct|id, data = df_long)

summary(model_int_slope)


anova(model_int, model_int_slope)

```



## Fit final model with REML

Now, we fit the data with REML instead of ML. All estimates of the predictors are significant. We also calculate the Intra-class correlation (perc of var explained)

```{r}
model_final <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
                         random = ~1|id, data = df_long)


model_final_lmer <- lme4::lmer(ndi ~ time_fct + hads_tot_cnt + (1|id), REML = T, data = df_long)
summary(model_final)
final_mod_summ <- coef(summary(model_final))
xtable(final_mod_summ)


```

## Calculate the Intra-Class Correlation (ICC)

```{r}

model_icc <- nlme::lme(ndi ~ 1, method = "REML", random = ~1|id, data = df_long)
summary(model_icc)

model_icc_lmer <- lme4::lmer(ndi ~ 1 + (1 |id), REML = T, data = df_long)
summary(model_icc_lmer)
# Calculate the intra class correlation:
# Model explains only 24 % of variance
random_effects <- VarCorr(model_icc)
print(random_effects,comp=c("Variance"))
102.9839 / ( 102.9839 + 331.8927)



```


## Plot random effects

```{r}

rr1 <- ranef(model_final_lmer,  condVar = T)
ggCaterpillar(rr1, QQ = F, likeDotplot = T)

rr2 <- ranef(model_icc_lmer,  condVar = T)
ggCaterpillar(rr2, QQ = F, likeDotplot = T)

df_long %>%
  filter(id == "L8061002" | id == "L8105022")




```

## Calculate estimated marginal means (EMM)

We calculate the EMM for NDI, taking into account the repeated measurements over time and the HADS score.

```{r}

refgrid <-  ref_grid(model_final)
refgrid

df_emmeans <-  data.frame(summary(refgrid))
df_emmeans$time <- as.numeric(as.character(df_emmeans$time_fct))
df_emmeans$lwr <- df_emmeans$prediction - 1.96*df_emmeans$SE
df_emmeans$upr <- df_emmeans$prediction + 1.96*df_emmeans$SE


```


Plot marginal NDI with CI over time

```{r}

# Plot only marginal mean
p3 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) + geom_point() + 
  geom_line(col = "blue", size = 2) + 
  geom_point(col = "blue", size = 4) +
  geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.3) + 
  theme(legend.position = "none") +
  ylab("NDI")  + ylim(c(0,50)) + 
  xlab("Time after surgery (weeks)") + theme

plot(p3)

# Plot marginal + raw data
p4 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) +  
  geom_point(aes(x=time, y = ndi, col = id), size = 4, alpha = 0.3, data = df_long) +
  geom_line(aes(x=time, y =ndi, group = id, col = id), alpha = 0.3, size = 1, data = df_long) +
  theme(legend.position = "none") +
  geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.7) +
  geom_line( col = "blue", size = 3) + 
  geom_point(col = "blue", size = 5) +
  ylab("NDI")  + ylim(c(0,100)) + 
  xlab("Time after surgery (weeks)") + theme

plot(p4)





png("Figures/Marginal_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p3)
dev.off()

png("Figures/Marginal&Raw_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p4)
dev.off()

```

# Cross validation

We use 5-fold Cross validation to calculate the Root Mean Squared Error (RMSE) = 13.2 and SD = 2. Meaning that on average the model makes an error of 13.2 NDI when predicting

```{r}

# Add new column with folds numbers
K <- 5
df_long <- fold(df_long, k = K, id_col = "id")


# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
  mse <- sum((x-y)^2)/length(x)
  rmse <- sqrt(mse)
  return(rmse)
}


loss <- numeric(K)
list_pred <- c()

for (k in 1:K){
  training <- df_long[df_long$.folds !=k, ]
  validation <- df_long[df_long$.folds ==k, ]
  training.fit <- lme4::lmer(ndi ~  time_fct + hads_tot_cnt + (1|id), 
                    data = training, REML = T)
  
  validation.predict <- predict(training.fit, newdata=validation, type='response',
                                 allow.new.levels = T)
  loss[k] <- Loss(validation$ndi, validation.predict)
  
  # Save predictions in list
  list_pred[[k]] <- validation %>%
    dplyr::select(id, ndi, time_fct, .folds) %>%
    mutate(pred = validation.predict)
}


# Show RMSE + SD
loss
RMSE  <- round(mean(loss), digits = 1)
RMSE
round(sd(loss), digits = 1)


# 95 % CI of RMSE
RMSE_se <- sd(loss) / sqrt(100)
lower <- round(mean(loss) - 1.96 * RMSE_se, digits = 1)
upper  <- round(mean(loss) + 1.96 * RMSE_se, digits = 1)
c(lower, upper)

```




```{r}

# Create folds
K <- 5
index <- rep(1:K, floor(nrow(df_long)/K)+1)[1:nrow(df_long)]
fold.index <- sample(index)


# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
  mse <- sum((x-y)^2)/length(x)
  rmse <- sqrt(mse)
  return(rmse)
}

loss <- numeric(K)

for (k in 1:K){
  training <- df_long[fold.index!=k, ]
  validation <- df_long[fold.index==k, ]
  training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id), 
                    data = training, REML = T)
  
  validation.predict <- predict(training.fit, newdata=validation, type='response',
                                 allow.new.levels = T)
  loss[k] <- Loss(validation$ndi, validation.predict)
}

loss
mean(loss)


```

