---
title: "Statistical Consulting Mixed Model Analysis"
author: "Ilse van Beelen & Floor Komen"
date: "11/4/2019"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE )
```


```{r, include=FALSE}
# Load ggCaterpillar, used to plot the random intercept


rm(list = ls())
library(ggplot2)
library(lme4)
library(tidyverse)
library(nlme)
library(emmeans)
library(car)
library(lattice)
library(xtable)
library(gridExtra)


ggCaterpillar <- function(re, QQ=TRUE, likeDotplot=TRUE) {
    require(ggplot2)
    f <- function(x) {
    pv   <- attr(x, "postVar")
    cols <- 1:(dim(pv)[1])
    se   <- unlist(lapply(cols, function(i) sqrt(pv[i, i, ])))
    ord  <- unlist(lapply(x, order)) + rep((0:(ncol(x) - 1)) * nrow(x), each=nrow(x))
    pDf  <- data.frame(y=unlist(x)[ord],
                       ci=1.96*se[ord],
                       nQQ=rep(qnorm(ppoints(nrow(x))), ncol(x)),
                       ID=factor(rep(rownames(x), ncol(x))[ord], levels=rownames(x)[ord]),
                       ind=gl(ncol(x), nrow(x), labels=names(x)))

    if(QQ) {  ## normal QQ-plot
        p <- ggplot(pDf, aes(nQQ, y))
        p <- p + facet_wrap(~ ind, scales="free")
        p <- p + xlab("Standard normal quantiles") + ylab("Random effect quantiles")
    } else {  ## caterpillar dotplot
        p <- ggplot(pDf, aes(ID, y)) + coord_flip()
        if(likeDotplot) {  ## imitate dotplot() -> same scales for random effects
            p <- p + facet_wrap(~ ind)
        } else {           ## different scales for random effects
            p <- p + facet_grid(ind ~ ., scales="free_y")
        }
        p <- p + xlab("Levels") + ylab("Random effects")
    }

    p <- p + theme(legend.position="none")
    p <- p + geom_hline(yintercept=0)
    p <- p + geom_errorbar(aes(ymin=y-ci, ymax=y+ci), width=0, colour="black")
    p <- p + geom_point(aes(size=1.2), colour="blue") 
    return(p)
    }
  lapply(re, f)
      
}

# Set layout for all figures
theme <- theme(panel.background = element_blank(),
          panel.grid.major = element_line(colour = "darkgrey", size=0.5),
          panel.grid.minor = element_line(colour = "grey", 
                                          size=.25, 
                                          linetype = "dashed"),
          panel.border = element_blank(),
          axis.line.x = element_line(colour = "black", 
                                     size=0.5, 
                                     lineend = "butt"),
          axis.line.y = element_line(colour = "black", 
                                     size=0.5),
          axis.text=element_text(size=15),
              axis.title=element_text(size=22),
              plot.title = element_text(size = 22),
              strip.text = element_text(size = 15),
              legend.title = element_blank())


```



```{r, results='hide'}

# Load libraries and read in data


df_long <- read.csv("Data/data_final_2019-11-05.csv", sep = ",")

df_long <- df_long %>% 
  mutate(hads_tot = hads_depr + hads_anx) %>%
  select(id, ndi, time, hads_tot, hads_anx, hads_depr ) %>%
  mutate(ndi = as.numeric(ndi), time = as.numeric(time),
         hads_tot = as.numeric(hads_tot), time_fct = as.factor(time))
  



```


# Correlation plots

We create a new dataframe where we have a seperate column for NDI after 0, 1 and 2 years

```{r, results='hide'}

df_corr <- df_long %>%
  select(id, time, ndi) %>%
  #gather(variable, value, -c(ndi:hads_depr)) %>%
  #unite(temp, student, variable) %>%
  #spread(temp, value)
  spread(key = time, value = ndi, sep = "_ndi")

df_corr2 <- df_long %>%
  select(id, time, hads_tot) %>%
  #gather(variable, value, -c(ndi:hads_depr)) %>%
  #unite(temp, student, variable) %>%
  #spread(temp, value)
  spread(key = time, value = hads_tot, sep = "_hads")


df_corr <- left_join(df_corr, df_corr2, "id")

```


```{r}

p1 <- ggplot(data = df_corr, aes(x = time_ndi0 , y = time_ndi52)) +  
  geom_point(size = 4) + geom_smooth(method=lm, se = F, size = 1.5) + 
  #geom_text(x = 5, y = 80, label = lm_eqn(df_long, x = "hads_anx", y = "ndi"), parse = TRUE) + 
  xlab("NDI score baseline") + ylab("NDI score after 1 years") + theme

p2 <- ggplot(data = df_corr, aes(x = time_ndi0 , y = time_ndi104)) +  
  geom_point(size = 4) + geom_smooth(method=lm, se = F, size = 1.5) + 
  #geom_text(x = 5, y = 80, label = lm_eqn(df_long, x = "hads_anx", y = "ndi"), parse = TRUE) + 
  xlab("NDI score baseline") + ylab("NDI score after 2 years") + theme

cor.test(df_corr$time_ndi0, df_corr$time_ndi52)
cor.test(df_corr$time_ndi0, df_corr$time_ndi104)

p1p2 <- grid.arrange(p1, p2, nrow=1)

png("Figures/Corr_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p1p2)
dev.off()


```

# Multiple linear model

```{r}

lm1 <- lm(time_ndi104 ~ time_ndi0 + time_hads0, data = df_corr)
summary(lm1)

```



# Linear Mixed Model Analysis

We tested several models. Statistically speaking model4 is best, but we choose the parameters based on clinical importance. Therefore, model 5 is choosen.

```{r}

model1 <- nlme::lme(ndi ~ time_fct, random = ~1|id, method= "ML", data = df_long)
model2 <- nlme::lme(ndi ~ time_fct + hads_tot, random = ~1|id, method= "ML", data = df_long)
model3 <- nlme::lme(ndi ~ time_fct + hads_tot + time:hads_tot, random = ~1|id, method= "ML", 
                    data = df_long)
model4 <- nlme::lme(ndi ~ time_fct  + hads_depr, random = ~1|id, method= "ML", 
                    data = df_long)
model5 <- nlme::lme(ndi ~ time_fct + hads_anx + hads_depr, random = ~1|id, method= "ML", 
                    data = df_long)

anova(model1, model2)
anova(model2, model3)
anova(model4, model5)
summary(model5)



```

## Check multicollinearity

There is a strong correlation between HADS anxiety and HADS depression. We now check for multicollinearity. No VIF above 5 (thus nu R-squared above 0.9)

```{r}
vif(model5)

```


## Check assumptions model5

We assume the folowing:

- error term is normally distributed
- error term is homogeneously distributed
- no outliers

```{r}

# Homogeneity of variance
par(mfrow = (c(1,3)))
plot(resid(model5))
df_long[c(34, 289), ]

# Normally distributed residuals
qqPlot(resid(model5))

# Influencial points
infl2 = influence(model5, obs = T)
cooksd = cooks.distance(infl2) # two outliers, 


plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")  # plot cook's distance
abline(h = 4*mean(cooksd, na.rm=T), col="red")  # add cutoff line
text(x = 1:length(cooksd) , y=cooksd, labels=ifelse(cooksd> 4*mean(cooksd, na.rm = T),
                                                    1:length(cooksd),""), col="blue", pos =2)

df_long[c(289, 27, 287, 143), ]



```


## Best fitting is model5

Now, we fit the data with REML instead of ML. All estimates of the predictors are significant, except HADS anxiety. We also calculate the Intra-class correlation (perc of var explained)

```{r}

#model_final <- nlme::lme(ndi ~ time + hads_tot, random = ~1|id, method= "REML", data = df_long)
model_final <- nlme::lme(ndi ~ time_fct + hads_anx + hads_depr, method = "ML",
                         random = ~1|id, data = df_long)
summary(model_final)
final_mod_summ <- coef(summary(model_final))
xtable(final_mod_summ)


# Calculate the intra class correlation:
# Model explains only 47.3 % of variance
random_effects <- VarCorr(model_final)
print(random_effects,comp=c("Variance"))
79.54823 / (79.54823 + 88.6839)

```


## Plot random effects

```{r}


rr1 <- ranef(model_final,  condVar = T)
ggCaterpillar(rr1, QQ = F, likeDotplot = T)

```

## Calculate estimated marginal means (EMM)

We calculate the EMM for NDI, taking into account the repeated measurements over time and the HADS score.

```{r}

refgrid <-  ref_grid(model_final)
refgrid

df_emmeans <-  data.frame(summary(refgrid))
df_emmeans$time <- as.numeric(as.character(df_emmeans$time_fct))
df_emmeans$lwr <- df_emmeans$prediction - 1.96*df_emmeans$SE
df_emmeans$upr <- df_emmeans$prediction + 1.96*df_emmeans$SE


```


Plot average NDI with CI over time

```{r}

p3 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) + geom_point() + 
  geom_line(col = "blue", size = 2) + 
  geom_point(col = "blue", size = 4) +
  geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.3) + 
  theme(legend.position = "none") +
  ylab("NDI")  +
  xlab("Time (weeks)") + theme


png("Figures/Marginal_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p3)
dev.off()

```

## Cross validation

```{r}

# Create folds
K <- 5
index <- rep(1:K, floor(nrow(df_long)/K)+1)[1:nrow(df_long)]
fold.index <- sample(index)


# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
  mse <- sum((x-y)^2)/length(x)
  rmse <- sqrt(mse)
  return(rmse)
}

loss <- numeric(K)

for (k in 1:K){
  training <- df_long[fold.index!=k, ]
  validation <- df_long[fold.index==k, ]
  training.fit <- model_final
  
  validation.predict <- predict(training.fit, newdata=validation, type='response')
  loss[k] <- Loss(validation$ndi, validation.predict)
}

loss
mean(loss)


```

