---
title: 'Part6: Improve predictions'
author: "Ilse van Beelen & Floor Komen"
date: "2020/03/24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

Goal: Add new variables to improve predictions. Also use methods such as best subset ridge and lasso to improve predictions.

# Set-up

```{r}

rm(list = ls())
set.seed(19950306)

library(lme4)
library(tidyverse)
library(leaps)

df_pred <- read.csv("Data/df_prediction_new_2020-03-23.csv")

# VINT01A, VINT01J, VINT01W, OSTSPOND --> to many NAs
# X, id_revalue, ndi0, hads0_tot, hads0_anx, hads0_depr --> not used
df_pred <- df_pred %>%
  dplyr::select(-c(X, id_revalue, ndi0, hads0_tot, hads0_anx, hads0_depr, VINT01A, VINT01J, VINT01W,
                   OSTSPOND))

# remove all missing values
#df_pred2 <- df_pred%>% filter_all(any_vars(is.na(.)))

# create dataframe for LM (only for time = 52 weeks)
df_pred_lm <- df_pred %>%
  dplyr::filter(time_fct == 52) %>%
  dplyr::select(-c(id, time_fct)) %>% # dont need these variables
  na.omit() 


# create train (80%) and test (20%) data for best subset, ridge, lasso

shuffle_ids <- sample(c(0,1), nrow(df_pred_lm), prob = c(0.5, 0.5), replace = T)

df_train <- df_pred_lm %>% filter(shuffle_ids == 0) %>% na.omit()
df_test <- df_pred_lm %>% filter(shuffle_ids == 1) %>% na.omit()



```

# Missing values

```{r}

na_count <- sapply(df_pred, function(y) sum(length(which(is.na(y)))))
na_count

round(na_count/nrow(df_pred), 2)

```


# LM

```{r}

mdl_full <- lm(ndi1_2 ~ ., data = df_pred_lm)
summary(mdl_full)

```


# LM - Best subset

```{r}
library(tidyr)
# remove 3 subjects to make number even (90/5=18)
#df_pred_lm <- df_pred_lm[-c(91:93),]


K <- 5
index <- rep(1:K, floor(nrow(df_pred_lm)/K)+1)[1:nrow(df_pred_lm)]
fold.index <- sample(index)
table(fold.index)


# Apply CV for best subset
prediction_subsets <- function(model, new_data, id){
  form <- as.formula(model$call [[2]])
  mat <- model.matrix(form,new_data)
  
  coefi <- coef(model ,id=id)
  xvars <- names(coefi)
  result <- mat[,xvars]%*%coefi
  
  return(result)
}


mse_train_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))
mse_test_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))

for(k in 1:K){
  training <- df_pred_lm[fold.index!=k, ]
  testing <- df_pred_lm[fold.index==k, ]
  
  best_fit <- regsubsets(ndi1_2 ~ ., data = training, nvmax = 16)
  training_no_na <- na.omit(training)
  testing_no_na <- na.omit(testing)
  for (i in 1:16){
    pred <-  prediction_subsets(best_fit, testing, id = i)
    pred_train <- prediction_subsets(best_fit, training, id=i)
    mse_train_subset[k, i] <-  mean((training_no_na$ndi1_2 - pred_train)^2)
    mse_test_subset[k, i] <-  mean((testing_no_na$ndi1_2 - pred)^2)
  }
}



# Average over the folds and choose 11 predictors
mse_train_subset <- apply(mse_train_subset, 2, mean)
mse_test_subset <- apply(mse_test_subset, 2, mean)
mse_train_subset
mse_test_subset #min mse is 168.124
which.min(mse_test_subset)

coef(best_fit, 1) #Best subset only has Intercept and ndi0_cnt
```


```{r}

# EXTRA CODE
cv.errors <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))
folds <- fold.index

for(j in 1:K){
  # Fit the model with each subset of predictors on the training part of the fold
  best.fit=regsubsets(ndi1_2 ~.,data=df_pred_lm[folds!=j,], nvmax=15) 
  # For each subset
  for(i in 1:16){
    # Predict on the hold out part of the fold for that subset
    pred=prediction_subsets(best.fit, df_pred_lm[folds==j,],id=i)
    # Get the mean squared error for the model trained on the fold with the subset
    cv.errors[j,i]=mean((df_pred_lm$ndi1_2[folds==j]-pred)^2)
  }
}


```


# LM - Ridge
```{r}
library(glmnet)


x_train <- model.matrix(ndi1_2~.,data=df_train)[,-1]
y_train <- df_train$ndi1_2
x_test <- model.matrix(ndi1_2~.,data=df_test)[,-1]
y_test <- df_test$ndi1_2
grid <- 10^seq(10,-2, length =100)

#implement the model and find the right lambda
ridge_MSE <- matrix(data=numeric(5*100),nrow=5,ncol=100)



ridge.mod <- glmnet(x_train, y_train, alpha=0, lambda=grid, standardize=TRUE)
cv.out.ridge <- cv.glmnet(x_train,y_train,alpha=0)
bestlam.ridge <- cv.out.ridge$lambda.min






mse_ridge_CV <- numeric(5)
for(j in 1:5){
  lm(ndi1_2 ~ ., data = df_pred_lm)
  x_train <- model.matrix(ndi1_2~.,data= df_pred_lm[fold.index != j,])[,-1]
  y_train <- na.omit(df_pred_lm[fold.index != j,]$ndi1_2)
  x_test <- model.matrix(ndi1_2~.,data=df_pred_lm[fold.index == j, ])[,-1]
  y_test <- na.omit(df_pred_lm[fold.index == j, ]$ndi1_2)
  ridge.mod <- glmnet(x_train, y_train, alpha=0, lambda=grid, standardize=TRUE)
  pred.ridge <- predict(ridge.mod,s=bestlam.ridge,newx=x_test)
  mse_ridge_CV[j] <-mean((pred.ridge -y_test)^2)
}
mean(mse_ridge_CV)#166.33

coef(ridge.mod, s = bestlam.ridge)
```


# LM - Lasso
```{r}
x_train <- model.matrix(ndi1_2~.,data=df_train)[,-1]
y_train <- df_train$ndi1_2
x_test <- model.matrix(ndi1_2~.,data=df_test)[,-1]
y_test <- df_test$ndi1_2
grid <- 10^seq(10,-2, length =100)

#implement the model and find the right lambda
lasso_MSE <- matrix(data=numeric(5*100),nrow=5,ncol=100)



lasso.mod <- glmnet(x_train, y_train, alpha=1, lambda=grid, standardize=TRUE)
cv.out.lasso<- cv.glmnet(x_train,y_train,alpha=1)
bestlam.lasso <- cv.out.lasso$lambda.min






mse_lasso_CV <- numeric(5)
for(j in 1:5){
  lm(ndi1_2 ~ ., data = df_pred_lm)
  x_train <- model.matrix(ndi1_2~.,data= df_pred_lm[fold.index != j,])[,-1]
  y_train <- na.omit(df_pred_lm[fold.index != j,]$ndi1_2)
  x_test <- model.matrix(ndi1_2~.,data=df_pred_lm[fold.index == j, ])[,-1]
  y_test <- na.omit(df_pred_lm[fold.index == j, ]$ndi1_2)
  lasso.mod <- glmnet(x_train, y_train, alpha=1, lambda=grid, standardize=TRUE)
  pred.lasso <- predict(lasso.mod,s=bestlam.lasso,newx=x_test)
  mse_lasso_CV[j] <-mean((pred.lasso -y_test)^2)
}
mean(mse_lasso_CV)#163.811

coef(lasso.mod, s = bestlam.lasso)
```

# LMM

```{r}

model_final <- nlme::lme(ndi1_2 ~ ndi0_cnt + hads0_tot_cnt + time_fct + AGE.OK + 
                           NEKPIJN + NEKHFAM + WEIGHT , method = "REML",
                         random = ~1|id, data = df_pred2, na.action = na.omit)

summary(model_final)


model_full <- nlme::lme(ndi1_2 ~ ., method = "REML",
                         random = ~1|id, data = df_pred2,  na.action = na.omit) # werkt niet

summ ary(model_full)

```






