---
title: 'Part6: Improve predictions'
author: "Ilse van Beelen & Floor Komen"
date: "2020/03/24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

Goal: Add new variables to improve predictions. Also use methods such as best subset ridge and lasso to improve predictions.

# Set-up

```{r}

rm(list = ls())
set.seed(19950306)

library(lme4)
library(tidyverse)
library(leaps)
library(glmnet)
library(lmmlasso)
library(lmmen)
library(MASS)
library(groupdata2)

df_pred <- read.csv("Data/df_prediction_new_2020-03-23.csv")



# VINT01A, VINT01J, VINT01W, OSTSPOND --> to many NAs
# X, id_revalue, ndi0, hads0_tot, hads0_anx, hads0_depr --> not used
df_pred <- df_pred %>%
  dplyr::select(-c(X, ndi0, hads0_tot, hads0_anx, hads0_depr, VINT01A, VINT01J, VINT01W,
                   OSTSPOND)) %>%
  mutate(ndi1_2 = as.numeric(ndi1_2)) %>%
  na.omit(.) # remove NAs

# create dataframe for LM (only for time = 52 weeks)
df_pred_lm <- df_pred %>%
  dplyr::filter(time_fct == 52) %>%
  dplyr::select(-c(time_fct, id_revalue)) %>% # dont need these variables
  na.omit(.) 



# create train (80%) and test (20%) data for best subset, ridge, lasso
shuffle_ids <- sample(c(0,1), nrow(df_pred_lm), prob = c(0.5, 0.5), replace = T)
df_train <- df_pred_lm %>% filter(shuffle_ids == 0) %>% na.omit()
df_test <- df_pred_lm %>% filter(shuffle_ids == 1) %>% na.omit()



```




# Missing values

The predictors VINT01A, VINT01J, VINT01W, OSTSPOND contain very high amounts of NAs. Therefore, these were already removed. The remaining predictors contain betweeen 0-13 \% NAs.

```{r}

na_count <- sapply(df_pred, function(y) sum(length(which(is.na(y)))))
round(na_count/nrow(df_pred), 2)


```


# LM

```{r}

mdl_full <- lm(ndi1_2 ~ ., data = df_pred_lm)
summary(mdl_full)

```


# LM - Best subset

```{r}

K <- 5
index <- rep(1:K, floor(nrow(df_pred_lm)/K)+1)[1:nrow(df_pred_lm)]
fold.index <- sample(index)
table(fold.index)


# Apply CV for best subset
prediction_subsets <- function(model, new_data, id){
  form <- as.formula(model$call [[2]])
  mat <- model.matrix(form,new_data)
  
  coefi <- coef(model ,id=id)
  xvars <- names(coefi)
  result <- mat[,xvars]%*%coefi
  
  return(result)
}


mse_train_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))
mse_test_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))

for(k in 1:K){
  training <- df_pred_lm[fold.index!=k, ]
  testing <- df_pred_lm[fold.index==k, ]
  
  best_fit <- regsubsets(ndi1_2 ~ ., data = training, nvmax = 16)
  training_no_na <- na.omit(training)
  testing_no_na <- na.omit(testing)
  for (i in 1:16){
    pred <-  prediction_subsets(best_fit, testing, id = i)
    pred_train <- prediction_subsets(best_fit, training, id=i)
    mse_train_subset[k, i] <-  mean((training_no_na$ndi1_2 - pred_train)^2)
    mse_test_subset[k, i] <-  mean((testing_no_na$ndi1_2 - pred)^2)
  }
}



# Average over the folds and choose 11 predictors
mse_train_subset <- apply(mse_train_subset, 2, mean)
mse_test_subset <- apply(mse_test_subset, 2, mean)
mse_train_subset
mse_test_subset #min mse is 168.124
which.min(mse_test_subset)

coef(best_fit, 1) #Best subset only has Intercept and ndi0_cnt
```




# LM - Ridge

```{r}
x_train <- model.matrix(ndi1_2~.,data=df_train)[,-1]
y_train <- df_train$ndi1_2
x_test <- model.matrix(ndi1_2~.,data=df_test)[,-1]
y_test <- df_test$ndi1_2
grid <- 10^seq(10,-2, length =100)

#implement the model and find the right lambda
ridge_MSE <- matrix(data=numeric(5*100),nrow=5,ncol=100)



ridge.mod <- glmnet(x_train, y_train, alpha=0, lambda=grid, standardize=TRUE)
cv.out.ridge <- cv.glmnet(x_train,y_train,alpha=0)
bestlam.ridge <- cv.out.ridge$lambda.min






mse_ridge_CV <- numeric(5)
for(j in 1:5){
  lm(ndi1_2 ~ ., data = df_pred_lm)
  x_train <- model.matrix(ndi1_2~.,data= df_pred_lm[fold.index != j,])[,-1]
  y_train <- na.omit(df_pred_lm[fold.index != j,]$ndi1_2)
  x_test <- model.matrix(ndi1_2~.,data=df_pred_lm[fold.index == j, ])[,-1]
  y_test <- na.omit(df_pred_lm[fold.index == j, ]$ndi1_2)
  ridge.mod <- glmnet(x_train, y_train, alpha=0, lambda=grid, standardize=TRUE)
  pred.ridge <- predict(ridge.mod,s=bestlam.ridge,newx=x_test)
  mse_ridge_CV[j] <-mean((pred.ridge -y_test)^2)
}
mean(mse_ridge_CV)#166.33

coef(ridge.mod, s = bestlam.ridge)
```


# LM - Lasso
```{r}
x_train <- model.matrix(ndi1_2~.,data=df_train)[,-1]
y_train <- df_train$ndi1_2
x_test <- model.matrix(ndi1_2~.,data=df_test)[,-1]
y_test <- df_test$ndi1_2
grid <- 10^seq(10,-2, length =100)

#implement the model and find the right lambda
lasso_MSE <- matrix(data=numeric(5*100),nrow=5,ncol=100)



lasso.mod <- glmnet(x_train, y_train, alpha=1, lambda=grid, standardize=TRUE)
plot(lasso.mod)
cv.out.lasso<- cv.glmnet(x_train,y_train,alpha=1)
plot(cv.out.lasso)
bestlam.lasso <- cv.out.lasso$lambda.min




mse_lasso_CV <- numeric(5)
for(j in 1:5){
  lm(ndi1_2 ~ ., data = df_pred_lm)
  x_train <- model.matrix(ndi1_2~.,data= df_pred_lm[fold.index != j,])[,-1]
  y_train <- na.omit(df_pred_lm[fold.index != j,]$ndi1_2)
  x_test <- model.matrix(ndi1_2~.,data=df_pred_lm[fold.index == j, ])[,-1]
  y_test <- na.omit(df_pred_lm[fold.index == j, ]$ndi1_2)
  lasso.mod <- glmnet(x_train, y_train, alpha=1, lambda=grid, standardize=TRUE)
  pred.lasso <- predict(lasso.mod,s=bestlam.lasso,newx=x_test)
  mse_lasso_CV[j] <-mean((pred.lasso -y_test)^2)
}
mean(mse_lasso_CV)#163.811

coef(lasso.mod, s = bestlam.lasso)
```




