vif(model5)
model_final <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
random = ~1|id, data = df_long)
df_long <- fold(df_long, k = K, id_col = "id_revalue") %>%
print(n=Inf)
library(groupdata2)
# Add new column with folds numbers
K <- 5
df_long <- fold(df_long, k = K, id_col = "id_revalue") %>%
print(n=Inf)
df_long <- fold(df_long, k = K, id_col = "id")
View(df_long)
# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
mse <- sum((x-y)^2)/length(x)
rmse <- sqrt(mse)
return(rmse)
}
loss <- numeric(K)
list_pred <- c()
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <-  lme4::lmer(ndi1_2 ~ ndi0 + hads0_anx + hads0_depr + time_fct + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi1_2, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id_revalue, id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
loss <- numeric(K)
list_pred <- c()
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi1_2, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id_revalue, id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id_revalue, id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
View(df_long)
for (k in 1:K){
training <- df_prediction[df_prediction$.folds !=k, ]
validation <- df_prediction[df_prediction$.folds ==k, ]
training.fit <-  lme4::lmer(ndi1_2 ~ ndi0 + hads0_anx + hads0_depr + time_fct + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi1_2, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
loss <- numeric(K)
list_pred <- c()
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id_revalue, id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_anx + hads_depr + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id, ndi, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
# Show RMSE + SD
loss
RMSE  <- round(mean(loss), digits = 1)
RMSE
round(sd(loss), digits = 1)
# Load ggCaterpillar, used to plot the random intercept
rm(list = ls())
set.seed(19950306)
library(ggplot2)
library(lme4)
library(tidyverse)
library(nlme)
library(emmeans)
library(car)
library(lattice)
library(xtable)
library(gridExtra)
library(groupdata2)
ggCaterpillar <- function(re, QQ=TRUE, likeDotplot=TRUE) {
require(ggplot2)
f <- function(x) {
pv   <- attr(x, "postVar")
cols <- 1:(dim(pv)[1])
se   <- unlist(lapply(cols, function(i) sqrt(pv[i, i, ])))
ord  <- unlist(lapply(x, order)) + rep((0:(ncol(x) - 1)) * nrow(x), each=nrow(x))
pDf  <- data.frame(y=unlist(x)[ord],
ci=1.96*se[ord],
nQQ=rep(qnorm(ppoints(nrow(x))), ncol(x)),
ID=factor(rep(rownames(x), ncol(x))[ord], levels=rownames(x)[ord]),
ind=gl(ncol(x), nrow(x), labels=names(x)))
if(QQ) {  ## normal QQ-plot
p <- ggplot(pDf, aes(nQQ, y))
p <- p + facet_wrap(~ ind, scales="free")
p <- p + xlab("Standard normal quantiles") + ylab("Random effect quantiles")
} else {  ## caterpillar dotplot
p <- ggplot(pDf, aes(ID, y)) + coord_flip()
if(likeDotplot) {  ## imitate dotplot() -> same scales for random effects
p <- p + facet_wrap(~ ind)
} else {           ## different scales for random effects
p <- p + facet_grid(ind ~ ., scales="free_y")
}
p <- p + xlab("Levels") + ylab("Random effects")
}
p <- p + theme(legend.position="none")
p <- p + geom_hline(yintercept=0)
p <- p + geom_errorbar(aes(ymin=y-ci, ymax=y+ci), width=0, colour="black")
p <- p + geom_point(aes(size=1.2), colour="blue")
return(p)
}
lapply(re, f)
}
# Set layout for all figures
theme <- theme(panel.background = element_blank(),
panel.grid.major = element_line(colour = "darkgrey", size=0.5),
panel.grid.minor = element_line(colour = "grey",
size=.25,
linetype = "dashed"),
panel.border = element_blank(),
axis.line.x = element_line(colour = "black",
size=0.5,
lineend = "butt"),
axis.line.y = element_line(colour = "black",
size=0.5),
axis.text=element_text(size=15),
axis.title=element_text(size=22),
plot.title = element_text(size = 22),
strip.text = element_text(size = 15),
legend.title = element_blank())
# Load libraries and read in data
df_long <- read.csv("Data/data_final_2019-11-05.csv", sep = ",")
# Create time factor & center HADS at its mean
df_long$time_fct <- as.factor(df_long$time_fct)
df_long$hads_tot_cnt <- df_long$hads_tot - mean(df_long$hads_tot)
str(df_long)
model_final <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
random = ~1|id, data = df_long)
summary(model_final)
# Add new column with folds numbers
K <- 5
df_long <- fold(df_long, k = K, id_col = "id")
# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
mse <- sum((x-y)^2)/length(x)
rmse <- sqrt(mse)
return(rmse)
}
loss <- numeric(K)
list_pred <- c()
training.fit <- lme4::lmer(ndi ~  time_fct + hads_tpt_cnt + (1|id),
data = training, REML = T)
loss <- numeric(K)
list_pred <- c()
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_tpt_cnt + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id, ndi, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
for (k in 1:K){
training <- df_long[df_long$.folds !=k, ]
validation <- df_long[df_long$.folds ==k, ]
training.fit <- lme4::lmer(ndi ~  time_fct + hads_tot_cnt + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id, ndi, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
# Show RMSE + SD
loss
RMSE  <- round(mean(loss), digits = 1)
RMSE
round(sd(loss), digits = 1)
# Load libraries
rm(list = ls())
set.seed(19950306)
library(ggplot2)
library(lme4)
library(tidyverse)
library(nlme)
library(emmeans)
library(car)
library(lattice)
library(merTools)
library(xtable)
library(caret)
library(groupdata2)
# Set layout for all figures
theme <- theme(panel.background = element_blank(),
panel.grid.major = element_line(colour = "darkgrey", size=0.5),
panel.grid.minor = element_line(colour = "grey",
size=.25,
linetype = "dashed"),
panel.border = element_blank(),
axis.line.x = element_line(colour = "black",
size=0.5,
lineend = "butt"),
axis.line.y = element_line(colour = "black",
size=0.5),
axis.text=element_text(size=15),
axis.title=element_text(size=22),
plot.title = element_text(size = 22),
strip.text = element_text(size = 15),
legend.title = element_blank())
df_prediction <- read.csv("Data/data_final_prediction_2019-11-05.csv", sep = ",")
# Center predictors -> makes interpretation intercept easier
# Create new numbering voor de IDs -> for Cross validation
df_prediction <- df_prediction %>%
mutate(id_revalue = as.factor(group_indices_(.,.dots=list("id")))) %>%
mutate(time_fct = as.factor(time_fct)) %>%
mutate(ndi0_cnt =  ndi0 - mean(ndi0)) %>%
mutate(hads0_tot_cnt = hads0_tot - mean(hads0_tot))
str(df_prediction)
df_prediction <- df_prediction %>%
drop_na(ndi1_2)
106*2
212 - 187
# Set layout for all figures
theme <- theme(panel.background = element_blank(),
panel.grid.major = element_line(colour = "darkgrey", size=0.5),
panel.grid.minor = element_line(colour = "grey",
size=.25,
linetype = "dashed"),
panel.border = element_blank(),
axis.line.x = element_line(colour = "black",
size=0.5,
lineend = "butt"),
axis.line.y = element_line(colour = "black",
size=0.5),
axis.text=element_text(size=15),
axis.title=element_text(size=22),
plot.title = element_text(size = 22),
strip.text = element_text(size = 15),
legend.title = element_blank())
df_prediction <- read.csv("Data/data_final_prediction_2019-11-05.csv", sep = ",")
# Center predictors -> makes interpretation intercept easier
# Create new numbering voor de IDs -> for Cross validation
df_prediction <- df_prediction %>%
mutate(id_revalue = as.factor(group_indices_(.,.dots=list("id")))) %>%
mutate(time_fct = as.factor(time_fct)) %>%
mutate(ndi0_cnt =  ndi0 - mean(ndi0)) %>%
mutate(hads0_tot_cnt = hads0_tot - mean(hads0_tot))
str(df_prediction)
df_prediction %>%
filter_all(is.na(.))
df_prediction %>%
filter_all(any_vars(is.na(.)))
df_prediction_with_na <- df_prediction %>%
filter_all(any_vars(is.na(.)))
df_prediction %>%
filter_all(any_vars(is.na(.)))
df_prediction_with_na <- df_prediction
df_prediction %>%
filter_all(any_vars(is.na(.)))
df_prediction <- df_prediction %>%
drop_na(ndi1_2)
# Add new column with folds numbers
K <- 5
df_prediction <- fold(df_prediction, k = K, id_col = "id_revalue") %>%
print(n=Inf)
# Create Loss function: Root Mean Squard Error
Loss <- function(x, y){
mse <- sum((x-y)^2)/length(x)
rmse <- sqrt(mse)
return(rmse)
}
loss <- numeric(K)
list_pred <- c()
for (k in 1:K){
training <- df_prediction[df_prediction$.folds !=k, ]
validation <- df_prediction[df_prediction$.folds ==k, ]
training.fit <-  lme4::lmer(ndi1_2 ~ ndi0 + hads0_anx + hads0_depr + time_fct + (1|id),
data = training, REML = T)
validation.predict <- predict(training.fit, newdata=validation, type='response',
allow.new.levels = T)
loss[k] <- Loss(validation$ndi1_2, validation.predict)
# Save predictions in list
list_pred[[k]] <- validation %>%
dplyr::select(id_revalue, id, ndi1_2, time_fct, .folds) %>%
mutate(pred = validation.predict)
}
# Show RMSE + SD
loss
RMSE  <- round(mean(loss), digits = 1)
RMSE
round(sd(loss), digits = 1)
df_cv <- do.call("rbind", list_pred)
df_cv52 <- df_cv %>%
filter(time_fct == 52)
Loss(df_cv52$pred, df_cv52$ndi1_2)
df_cv104 <- df_cv %>%
filter(time_fct == 104)
Loss(df_cv104$pred, df_cv104$ndi1_2)
df_cv52
View(list_pred)
tapply(df_cv52$pred, df_cv52$.folds, Loss)
tapply(df_cv52$pred, df_cv52$.folds, Loss(x, df_cv52$ndi1_2))
tapply(c(df_cv52$pred, df_cv52$ndi1_2), df_cv52$.folds, Loss)
df_cv52 %>%
group_by(.folds) %>%
mutate(loss = Loss(ndi1_2, pred))
df_cv52 <- df_cv52 %>%
group_by(.folds) %>%
mutate(loss = Loss(ndi1_2, pred))
unique(df_cv52$loss)
df_cv104 <- df_cv104 %>%
group_by(.folds) %>%
mutate(loss = Loss(ndi1_2, pred))
unique(df_cv104$loss)
mean(unique(df_cv104$loss))
mean(unique(df_cv52$loss))
round(mean(unique(df_cv52$loss)), 1)
rmse_52 <- unique(df_cv52$loss)
round(mean(rmse_52)), 1)
round(mean(rmse_52), 1)
round(sd(rmse_52), 1)
rmse_104 <- unique(df_cv104$loss)
rmse_104 <- unique(df_cv104$loss)
round(mean(rmse_104), 1)
round(sd(rmse_104), 1)
# Load ggCaterpillar, used to plot the random intercept
rm(list = ls())
set.seed(19950306)
library(ggplot2)
library(lme4)
library(tidyverse)
library(nlme)
library(emmeans)
library(car)
library(lattice)
library(xtable)
library(gridExtra)
library(groupdata2)
ggCaterpillar <- function(re, QQ=TRUE, likeDotplot=TRUE) {
require(ggplot2)
f <- function(x) {
pv   <- attr(x, "postVar")
cols <- 1:(dim(pv)[1])
se   <- unlist(lapply(cols, function(i) sqrt(pv[i, i, ])))
ord  <- unlist(lapply(x, order)) + rep((0:(ncol(x) - 1)) * nrow(x), each=nrow(x))
pDf  <- data.frame(y=unlist(x)[ord],
ci=1.96*se[ord],
nQQ=rep(qnorm(ppoints(nrow(x))), ncol(x)),
ID=factor(rep(rownames(x), ncol(x))[ord], levels=rownames(x)[ord]),
ind=gl(ncol(x), nrow(x), labels=names(x)))
if(QQ) {  ## normal QQ-plot
p <- ggplot(pDf, aes(nQQ, y))
p <- p + facet_wrap(~ ind, scales="free")
p <- p + xlab("Standard normal quantiles") + ylab("Random effect quantiles")
} else {  ## caterpillar dotplot
p <- ggplot(pDf, aes(ID, y)) + coord_flip()
if(likeDotplot) {  ## imitate dotplot() -> same scales for random effects
p <- p + facet_wrap(~ ind)
} else {           ## different scales for random effects
p <- p + facet_grid(ind ~ ., scales="free_y")
}
p <- p + xlab("Levels") + ylab("Random effects")
}
p <- p + theme(legend.position="none")
p <- p + geom_hline(yintercept=0)
p <- p + geom_errorbar(aes(ymin=y-ci, ymax=y+ci), width=0, colour="black")
p <- p + geom_point(aes(size=1.2), colour="blue")
return(p)
}
lapply(re, f)
}
# Set layout for all figures
theme <- theme(panel.background = element_blank(),
panel.grid.major = element_line(colour = "darkgrey", size=0.5),
panel.grid.minor = element_line(colour = "grey",
size=.25,
linetype = "dashed"),
panel.border = element_blank(),
axis.line.x = element_line(colour = "black",
size=0.5,
lineend = "butt"),
axis.line.y = element_line(colour = "black",
size=0.5),
axis.text=element_text(size=15),
axis.title=element_text(size=22),
plot.title = element_text(size = 22),
strip.text = element_text(size = 15),
legend.title = element_blank())
# Load libraries and read in data
df_long <- read.csv("Data/data_final_2019-11-05.csv", sep = ",")
# Create time factor & center HADS at its mean
df_long$time_fct <- as.factor(df_long$time_fct)
df_long$hads_tot_cnt <- df_long$hads_tot - mean(df_long$hads_tot)
str(df_long)
model_final <- nlme::lme(ndi ~ time_fct + hads_tot_cnt, method = "REML",
random = ~1|id, data = df_long)
summary(model_final)
final_mod_summ <- coef(summary(model_final))
xtable(final_mod_summ)
refgrid <-  ref_grid(model_final)
refgrid
df_emmeans <-  data.frame(summary(refgrid))
df_emmeans$time <- as.numeric(as.character(df_emmeans$time_fct))
df_emmeans$lwr <- df_emmeans$prediction - 1.96*df_emmeans$SE
df_emmeans$upr <- df_emmeans$prediction + 1.96*df_emmeans$SE
# Plot only marginal mean
p3 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) + geom_point() +
geom_line(col = "blue", size = 2) +
geom_point(col = "blue", size = 4) +
geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.3) +
theme(legend.position = "none") +
ylab("NDI")  + ylim(c(0,50)) +
xlab("Time (weeks)") + theme
plot(p3)
# Plot marginal + raw data
p4 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) +
geom_point(aes(x=time, y = ndi, col = id), size = 4, alpha = 0.3, data = df_long) +
geom_line(aes(x=time, y =ndi, group = id, col = id), alpha = 0.3, size = 1, data = df_long) +
theme(legend.position = "none") +
geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.7) +
geom_line( col = "blue", size = 3) +
geom_point(col = "blue", size = 5) +
ylab("NDI")  + ylim(c(0,100)) +
xlab("Time (weeks)") + theme
plot(p4)
# Plot only marginal mean
p3 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) + geom_point() +
geom_line(col = "blue", size = 2) +
geom_point(col = "blue", size = 4) +
geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.3) +
theme(legend.position = "none") +
ylab("NDI")  + ylim(c(0,50)) +
xlab("Time after surgery (weeks)") + theme
plot(p3)
# Plot marginal + raw data
p4 <- ggplot(data = df_emmeans, aes(x = time, y = prediction )) +
geom_point(aes(x=time, y = ndi, col = id), size = 4, alpha = 0.3, data = df_long) +
geom_line(aes(x=time, y =ndi, group = id, col = id), alpha = 0.3, size = 1, data = df_long) +
theme(legend.position = "none") +
geom_ribbon(data=df_emmeans, aes(ymin= lwr, ymax= upr), alpha=0.7) +
geom_line( col = "blue", size = 3) +
geom_point(col = "blue", size = 5) +
ylab("NDI")  + ylim(c(0,100)) +
xlab("Time after surgery (weeks)") + theme
plot(p4)
png("Figures/Marginal_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p3)
dev.off()
png("Figures/Marginal&Raw_NDI.png",width = 15, height = 7, units='in',res=300)
plot(p4)
dev.off()
