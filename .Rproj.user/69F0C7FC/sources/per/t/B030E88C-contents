---
title: 'Part6: Improve predictions'
author: "Ilse van Beelen"
date: "2020/03/24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

Goal: Add new variables to improve predictions. Also use methods such as best subset ridge and lasso to improve predictions.

# Set-up

```{r}

rm(list = ls())
set.seed(19950306)

library(lme4)
library(tidyverse)
library(leaps)

df_pred <- read.csv("Data/df_prediction_new_2020-03-23.csv")

# VINT01A, VINT01J, VINT01W, OSTSPOND --> to many NAs
# X, id_revalue, ndi0, hads0_tot, hads0_anx, hads0_depr --> not used
df_pred <- df_pred %>%
  dplyr::select(-c(X, id_revalue, ndi0, hads0_tot, hads0_anx, hads0_depr, VINT01A, VINT01J, VINT01W,
                   OSTSPOND))

# remove all missing values
#df_pred2 <- df_pred%>% filter_all(any_vars(is.na(.)))

# create dataframe for LM (only for time = 52 weeks)
df_pred_lm <- df_pred %>%
  dplyr::filter(time_fct == 52) %>%
  dplyr::select(-c(id, time_fct)) # dont need these variables


# create train (80%) and test (20%) data for best subset, ridge, lasso
#shuffle_ids <- sample(c(0,1), nrow(df_pred_lm), prob = c(0.5, 0.5), replace = T)

#df_train <- df_pred_lm %>% filter(shuffle_ids == 0)

#df_test <- df_pred_lm %>% filter(shuffle_ids == 1)



```

# Missing values

```{r}

na_count <- sapply(df_pred, function(y) sum(length(which(is.na(y)))))
na_count

round(na_count/nrow(df_pred), 2)

```


# LM

```{r}

mdl_full <- lm(ndi1_2 ~ ., data = df_pred_lm)
summary(mdl_full)

```


# LM - Best subset

```{r}

# remove 3 subjects to make number even (90/5=18)
#df_pred_lm <- df_pred_lm[-c(91:93),]


K <- 5
index <- rep(1:K, floor(nrow(df_pred_lm)/K)+1)[1:nrow(df_pred_lm)]
fold.index <- sample(index)
table(fold.index)


# Apply CV for best subset
prediction_subsets <- function(model, new_data, id){
  form <- as.formula(model$call [[2]])
  mat <- model.matrix(form,new_data)
  
  coefi <- coef(model ,id=id)
  xvars <- names(coefi)
  result <- mat[,xvars]%*%coefi
  
  return(result)
}


mse_train_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))
mse_test_subset <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))

for(k in 1:K){
  training <- df_pred_lm[fold.index!=k, ]
  testing <- df_pred_lm[fold.index==k, ]
  
  best_fit <- regsubsets(ndi1_2 ~ ., data = training, nvmax = 16)
  
  for (i in 1:16){
    pred <-  prediction_subsets(best_fit, testing, id = i)
    pred_train <- prediction_subsets(best_fit, training, id=i)
    mse_train_subset[k, i] <-  mean((training$ndi1_2 - pred_train)^2)
    mse_test_subset[k, i] <-  mean((testing$ndi1_2 - pred)^2)
  }
}



# Average over the folds and choose 11 predictors
mse_train_subset <- apply(mse_train_subset, 2, mean)
mse_test_subset <- apply(mse_test_subset, 2, mean)
mse_train_subset
mse_test_subset
which.min(mse_test_subset)

```


```{r}

# EXTRA CODE
cv.errors <- matrix(NA, K, 16, dimnames=list(NULL, paste(1:16)))
folds <- fold.index

for(j in 1:K){
  # Fit the model with each subset of predictors on the training part of the fold
  best.fit=regsubsets(ndi1_2 ~.,data=df_pred_lm[folds!=j,], nvmax=15) 
  # For each subset
  for(i in 1:16){
    # Predict on the hold out part of the fold for that subset
    pred=prediction_subsets(best.fit, df_pred_lm[folds==j,],id=i)
    # Get the mean squared error for the model trained on the fold with the subset
    cv.errors[j,i]=mean((df_pred_lm$ndi1_2[folds==j]-pred)^2)
  }
}


```


# LM - Ridge


# LM - Lasso


# LMM

```{r}

model_final <- nlme::lme(ndi1_2 ~ ndi0_cnt + hads0_tot_cnt + time_fct + AGE.OK + 
                           NEKPIJN + NEKHFAM + WEIGHT , method = "REML",
                         random = ~1|id, data = df_pred2, na.action = na.omit)

summary(model_final)


model_full <- nlme::lme(ndi1_2 ~ ., method = "REML",
                         random = ~1|id, data = df_pred2,  na.action = na.omit) # werkt niet

summary(model_full)

```





# Full subset search

```{r}

# Perform full subset-search
mdl_subset <- regsubsets(MEDV ~ ., data = df_pred, nvmax = 26)
summary(model_subset)

# Get some fun statistics: adjusted R-squared, MSE
summary(model_subset)$adjr2
subsets_mse <- summary(model_subset)$rss / nrow(train_data)


# Apply CV for best subset
K <-  5
folds <-  sample(1:K, nrow(train_data),replace=TRUE)
table(folds)

prediction_subsets <- function(model, new_data, id){
  form <- as.formula(model$call [[2]])
  mat <- model.matrix(form,new_data)
  
  coefi <- coef(model ,id=id)
  xvars <- names(coefi)
  result <- mat[,xvars]%*%coefi
  
  return(result)
}


mse_train_subset <- matrix(NA,K, 13, dimnames=list(NULL, paste(1:13)))
mse_test_subset <- matrix(NA,K, 13, dimnames=list(NULL, paste(1:13)))

for(k in 1:K){
  training <- data[fold.index!=k, ]
  testing <- data[fold.index==k, ]
  
  best_fit <- regsubsets(MEDV ~., data= train_data, nvmax = 13)
  
  for (i in 1:13){
    pred <-  prediction_subsets(best_fit, testing, id = i)
    pred_train <- prediction_subsets(best_fit, training, id=i)
    mse_train_subset[k, i] <-  mean((training$MEDV - pred_train)^2)
    mse_test_subset[k, i] <-  mean((testing$MEDV - pred)^2)
  }
}

# Average over the folds and choose 11 predictors
mse_train_subset <- apply(mse_train_subset, 2, mean)
mse_test_subset <- apply(mse_test_subset, 2, mean)
mse_train_subset
mse_test_subset
which.min(mse_test_subset)

```

